{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "name": "10-1(colab).Korean-text-analysis-visualization-1-movie-data-scraping",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibk25244/su-ai/blob/master/10_1(colab)_Korean_text_analysis_visualization_1_movie_data_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTjRlT7O0HO",
        "colab_type": "text"
      },
      "source": [
        "- - -\n",
        "\n",
        "<font size=6 color='tomato'>한글 텍스트 분석 및 시각화 - <font color='royalblue'>1 단계 : 데이터 수집</font>   \n",
        "<font size=5 color='purple'>Korean Text Analysis & Visualization - <font color='forestgreen'>Step 1: Movie Review Data Scraping</font>\n",
        "\n",
        "* * *\n",
        "\n",
        "**<font size=4>박 진 수</font>** 교수  \n",
        "Intelligent Data Semantics Lab  \n",
        "Seoul National University\n",
        "\n",
        "- - -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FOwoEiZQ77q",
        "colab_type": "toc"
      },
      "source": [
        ">[텍스트 분석 절차](#scrollTo=oVPt_2OiO0HQ)\n",
        "\n",
        ">[첫 번째 리뷰 내용 보여주기](#scrollTo=RIPJCaetD6GH)\n",
        "\n",
        ">>[URL 구조 파악](#scrollTo=guPXvSTVO0HY)\n",
        "\n",
        ">>[웹 상의 데이터 받아오기 : urllib.request.urlopen](#scrollTo=291XzvYQO0HZ)\n",
        "\n",
        ">>[웹 객체 파싱하기 : bs4.BeautifulSoup](#scrollTo=eSvGuOQAO0Hh)\n",
        "\n",
        ">>[웹 페이지(HTML) 구조 파악 : 구글 크롬 브라우저](#scrollTo=O6b3KrfgO0Hn)\n",
        "\n",
        ">[첫 페이지의 리뷰 모두 출력하기](#scrollTo=xemtb62UO0H1)\n",
        "\n",
        ">[여러 페이지의 리뷰 저장하기](#scrollTo=yWk_5NgGO0H7)\n",
        "\n",
        ">>[각 페이지의 URL 구조 패턴](#scrollTo=L2TAZTacO0H8)\n",
        "\n",
        ">>[첫 다섯 페이지의 리뷰 저장하기](#scrollTo=OYfgWMEMO0H8)\n",
        "\n",
        ">[모든 페이지의 리뷰 저장하기](#scrollTo=W_JQFvWTO0IF)\n",
        "\n",
        ">[THE END](#scrollTo=DZbP1gVfO0IV)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVPt_2OiO0HQ",
        "colab_type": "text"
      },
      "source": [
        "# 텍스트 분석 절차"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Rlprr2NpJGgG"
      },
      "source": [
        "**<font size='+3'>1 단계 - 데이터 수집</font>**\n",
        "- 웹, SNS 등에서 분석에 필요한 데이터를 수집한다.\n",
        "- 이미 데이터가 있으면 이 단계는 생략할 수 있다.\n",
        "\n",
        "**2** 단계 - **데이터 전처리**\n",
        "- 자연어를 기계가 이해할 수 있는 인공어(artificial language)로 번역한다.\n",
        "- Tokenization, POS Tagging(형태소 분석), Pruning 등 \n",
        "\n",
        "**3** 단계 - **데이터 탐색**\n",
        "- 분석의 방향성을 제시하기 위해 전처리한 데이터를 탐색한다.\n",
        "- 일반적으로는 단어의 출현 빈도(frequency)를 기반으로 탐색한다.\n",
        "\n",
        "**4** 단계 - **데이터 분석**\n",
        "- 텍스트 데이터를 통해 유의미한 정보를 추출하는 분석을 수행한다.\n",
        "- 감성분석, 토픽모델, 머신러닝 등\n",
        "\n",
        "**5** 단계 - **인사이트 도출**\n",
        "- 경영 환경에서 효과적인 의사 결정에 도움을 줄 수 있는 인사이트를 도출한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nt39S5_FO0HR"
      },
      "source": [
        "'웹 스크래핑(web scraping)'이란 웹 상에 존재하는 컨텐츠를 수집하는 작업을 말한다. '웹 크롤링(web crawling)'이라고도 부른다. \n",
        "\n",
        "이를 위해서는 웹페이지를 구성하는 HTML의 구조를 이해하고 필요로 하는 정보만을 뽑아내는 것이 필수적이다.\n",
        "\n",
        "파이썬에서는 **BeautifulSoup**이라는 패키지를 이용하여 HTML을 파싱하고 필요한 정보를 얻을 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAXTdi3RO0HV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bs4\n",
        "print('BeautifulSoup version...:', bs4.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RIPJCaetD6GH"
      },
      "source": [
        "# 첫 번째 리뷰 내용 보여주기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EeDZ93w4JKIS"
      },
      "source": [
        "[<다음 영화>](https://movie.daum.net/)  웹사이트에서 영화 ['비긴 어게인(2013)'](https://movie.daum.net/moviedb/main?movieId=80780)의 리뷰와 평점을 크롤링한다.\n",
        "- [다음 영화 '비긴 어게인(2013)'](https://movie.daum.net/moviedb/main?movieId=80780)  \n",
        "- 여기 사용하는 예는 '비긴 어게인(2013)'이지만 URL만 변경한다면 <다음 영화> 웹사이트에서 본인이 원하는 어떤 영화의 리뷰와 평점도 크롤링할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pP8tIMDzO0HY"
      },
      "source": [
        "네티즌 평점 보기\n",
        "- '평점' 섹션을 클릭해 들어가면 시청자들이 남긴 평점과 리뷰를 볼 수 있다.\n",
        "- <https://movie.daum.net/moviedb/grade?movieId=80780>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "guPXvSTVO0HY"
      },
      "source": [
        "## URL 구조 파악"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cjzRfZNtO0HZ"
      },
      "source": [
        "URL 구조를 파악하기 위한 팁\n",
        "- URL 구조를 파악하기 위해 두 번째 페이지를 클릭해본다.\n",
        "- 보통 첫 번째 페이지는 메인 페이지와 연결되어 있기 때문에 URL 구조를 보는데 적합하지 않을 수 있다.\n",
        "- 따라서 URL 구조를 파악하기 위해서는 일반적으로 두 번째, 세 번째 페이지를 열어보변서 귀납(inductive reasoning)적으로 파악하면 도움이 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "291XzvYQO0HZ"
      },
      "source": [
        "## 웹 상의 데이터 받아오기 : **urllib.request.urlopen**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PVLR_SmJO0HZ"
      },
      "source": [
        "이제 URL 구조를 파악하였으니, URL을 통해 웹 상의 리뷰 데이터를 받아올 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zC_WmHFW9IEU",
        "colab": {}
      },
      "source": [
        "import urllib\n",
        "\n",
        "# '평점' 섹션의 첫 번째 페이지 URL이다.\n",
        "url = 'https://movie.daum.net/moviedb/grade?movieId=80780&type=netizen&page=1'\n",
        "\n",
        "# urlopen()으로 받아온 리뷰 데이터는 HTTPResponse 객체로 반환한다.\n",
        "response = urllib.request.urlopen(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2iX6cdhSO0Hc",
        "colab": {}
      },
      "source": [
        "type(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBhVswXqO0He",
        "colab": {}
      },
      "source": [
        "response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DTIGq4KjO0Hh"
      },
      "source": [
        "이 데이터는 **HTTPResponse** 객체 형식으로 되어있기 때문에 바로 사용할 수 없고 파싱(parsing)이라는 절차를 거쳐 우리가 직관적으로 이해할 수 있는 텍스트 형식으로 변환하여야 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eSvGuOQAO0Hh"
      },
      "source": [
        "## 웹 객체 파싱하기 : **bs4.BeautifulSoup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9pe1HljgO0Hi",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# BeautifulSoup() 클래스의 인스턴스로 웹 데이터를 파싱한다. \n",
        "# Parser는 일반적으로 많이 사용하는 'html.parser'를 사용한다.\n",
        "bs = BeautifulSoup(response, 'html.parser')  # 'html5lib'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GmjLsPnlO0Hk",
        "colab": {}
      },
      "source": [
        "type(bs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2QAoJEg8O0Hl",
        "colab": {}
      },
      "source": [
        "bs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O6b3KrfgO0Hn"
      },
      "source": [
        "## 웹 페이지(HTML) 구조 파악 : 구글 크롬 브라우저"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2qc_faU8O0Ho"
      },
      "source": [
        "가져올 리뷰 데이터 구조 파악\n",
        "- 웹 스크레핑을 위해 웹 페이지의 전체 구조를 파악할 필요는 없다.\n",
        "- 구글 크롬(chrome) 브라우저로 얻고자 하는 데이터를 선택하고 마우스 오른쪽 클릭 후 '검사'를 클릭하면 HTML 소스 코드를 볼 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gqL_H__yO0Hp"
      },
      "source": [
        "HTML 소스 코드 패턴 분석\n",
        "- HTML 소스 코드를 자세히 살펴보면 우리가 수집하려는 리뷰 텍스트는 반복된 형식을 보여주고 있다.\n",
        "\n",
        "```\n",
        "<p class=\"desc_review\">리뷰 내용</p>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfguwm2kp2Rk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 방법 1\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7_lokskp5XK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 방법 2\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXzyJhLyp8mC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 방법 3\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt6vhMTMp-1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 방법 4\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-pnAMjRGO0Hx",
        "colab": {}
      },
      "source": [
        "type(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lordRj7HqIx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 텍스트만 추출한 후 화이트스페이스(whitespace)를 제거하고 출력한다.\n",
        "# 첫 번째 리뷰의 내용이 없다면 공백으로 출력될 수도 있다.\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xemtb62UO0H1"
      },
      "source": [
        "# 첫 페이지의 리뷰 모두 출력하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DSSQPLxMO0H2"
      },
      "source": [
        "앞의 예에서는 첫 번째 리뷰만 찾아서 텍스트를 추출했는데, 여기서는 첫 페이지의 모든 리뷰를 리스트로 받아온 후 **for** 문을 활용해 각 리뷰를 꺼내 텍스트만 추출한다.\n",
        "- **find_all**() : 전달인자로 받은 태그를 모두 찾아서 리스트(list) 자료형으로 반환한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxITZu_dqnRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 웹 스크래핑에 필요한 모듈을 불러온다.\n",
        "from urllib.request import urlopen\n",
        "# from bs4 import BeautifulSoup\n",
        "# import urllib\n",
        "\n",
        "# '비긴 어게인(2013)' 영화 리뷰의 첫 페이지를 가져온다.\n",
        "# URL만 바꾸면 다른 영화 리뷰 데이터 수집할 수 있다.\n",
        "url = __TODO__ \n",
        "\n",
        "try:\n",
        "    response = urlopen(url)\n",
        "except urllib.error.HTTPError as err:\n",
        "    print('The server returned an HTTP error:', err)\n",
        "except urllib.error.URLError as err:\n",
        "    print('The server could not be found!', err)\n",
        "else:\n",
        "    bs = BeautifulSoup(response, 'html.parser')  # 'html5lib'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmG1Otmwqx1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 해당 페이지의 리뷰들을 추출한 후 리스트에 추가한다.\n",
        "# find_all(name, attrs, recursive, string, limit, **kwargs) 메소드를 통해 모든 리뷰를 추출한다.\n",
        "tag_reviews = bs.find_all(__TODO__)\n",
        "\n",
        "# --- 평점도 동일하게 작업한다.\n",
        "tag_grades = bs.find_all(__TODO__)\n",
        "\n",
        "\n",
        "if tag_reviews is not None:\n",
        "    reviews = [review.get_text(strip=True) for review in tag_reviews]\n",
        "\n",
        "if tag_grades is not None:\n",
        "    grades = [grade.get_text() for grade in tag_grades]\n",
        "\n",
        "# 결과를 출력한다.    \n",
        "for i, review in enumerate(reviews):\n",
        "    print(f'{grades[i]} - {review}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWk_5NgGO0H7"
      },
      "source": [
        "# 여러 페이지의 리뷰 저장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L2TAZTacO0H8"
      },
      "source": [
        "## 각 페이지의 URL 구조 패턴"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AnSUj4guO0H8"
      },
      "source": [
        "앞서 파악한 각 페이지의 URL 구조는 아래와 같다.\n",
        "- http://movie.daum.net/moviedb/grade?movieId=80780&type=netizen&page=1\n",
        "- http://movie.daum.net/moviedb/grade?movieId=80780&type=netizen&page=2\n",
        "- http://movie.daum.net/moviedb/grade?movieId=80780&type=netizen&page=3\n",
        "- ...\n",
        "- http://movie.daum.net/moviedb/grade?movieId=80780&type=netizen&page=n\n",
        "\n",
        "다른 구조는 모두 같지만 맨 뒤의 정수(**1**, **2**, **3**,...)만 달라지면서 페이지가 달라진다.\n",
        "\n",
        "그러므로, 여러 페이지의 리뷰에 접근하기 위해서는 맨 뒤의 정수만 바꿔가면서 페이지를 열면 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OYfgWMEMO0H8"
      },
      "source": [
        "## 첫 다섯 페이지의 리뷰 저장하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud_iIYA-rYUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- '비긴 어게인(2013)' 영화 리뷰의 첫 5페이지 리뷰를 모두 가져온다.\n",
        "# from urllib.request import urlopen\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "grades = []                # 태그를 제거한 리뷰 평점 전체를 담을 리스트를 초기화한다.\n",
        "reviews = []               # 태그를 제거한 리뷰 전체를 담을 리스트를 초기화한다.\n",
        "\n",
        "try:\n",
        "    # 첫 다섯 페이지의 리뷰를 출력하고 저장한다.\n",
        "    for i in range(5):\n",
        "        url = __TODO__\n",
        "        http = urlopen(url)\n",
        "        bs = BeautifulSoup(http, 'html.parser')  # 'html5lib'\n",
        "        \n",
        "        # 해당 페이지의 평점을 추출한다.\n",
        "        grades += [grade.get_text() for grade in bs.find_all(__TODO__)]\n",
        "        \n",
        "        # 해당 페이지의 리뷰를 추출한다.\n",
        "        reviews += [review.get_text(strip=True) for review in bs.find_all(__TODO__)]\n",
        "\n",
        "        # 진행 사항을 표시한다.\n",
        "        print(f'... {i + 1} 페이지 완료')\n",
        "    else:\n",
        "        print('=' * 7, 'Job completed!', '=' * 25)\n",
        "except urllib.error.HTTPError as err:\n",
        "    print('The server returned an HTTP error:', err)\n",
        "except urllib.error.URLError as err:\n",
        "    print('The server could not be found!', err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qocPCh0ernvU",
        "colab": {}
      },
      "source": [
        "# --- 가져온 내용을 파일로 저장한다.\n",
        "path = 'movie-reviews-5-pages.txt'\n",
        "\n",
        "# 리뷰 전체 내용을 텍스트 파일 쓰기 모드로 연다.\n",
        "with open(path, mode='w', encoding='utf-8') as file:\n",
        "  __TODO__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5QTyZKU-PkY",
        "colab": {}
      },
      "source": [
        "# ======= For Google Colaboratory ===============================\n",
        "# --- 수집한 데이터와 전처리한 데이터를 로컬 파일로 내려받기를 한다.\n",
        "from google.colab import files\n",
        "files.download(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W_JQFvWTO0IF"
      },
      "source": [
        "# 모든 페이지의 리뷰 저장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DxJDc6nUJgC7"
      },
      "source": [
        "먼저 총 리뷰 개수와 리뷰 페이지 수를 구하고 **for** 문을 통해서 앞서 작성한 리뷰와 평점 추출을 반복하자.\n",
        "\n",
        "**[주의] : 아래 코드는 모든 리뷰 페이지를 가져오기 때문에 다소 시간이 걸릴 수 있다.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWTf1aSzsxeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- '비긴 어게인(2013)' 영화 리뷰의 첫 페이지를 가져온다.\n",
        "# from urllib.request import urlopen\n",
        "# from bs4 import BeautifulSoup\n",
        "# import urllib\n",
        "\n",
        "# URL만 바꾸면 다른 영화 리뷰 데이터 수집할 수 있다.\n",
        "url = __TODO__ \n",
        "\n",
        "try:\n",
        "    http = urlopen(url)\n",
        "except urllib.error.HTTPError as err:\n",
        "    print('The server returned an HTTP error:', err)\n",
        "except urllib.error.URLError as err:\n",
        "    print('The server could not be found!', err)\n",
        "else:\n",
        "    bs = BeautifulSoup(http, 'html.parser')  # 'html5lib'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPW2YUS4s6QJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -- 리뷰를 작성한 전체 네티즌의 인원을 담고 있는 태그를 가져온다\n",
        "reviewers = __TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1u52DjStHXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 총 리뷰 개수를 10으로 나누어 페이지 개수를 계산할 수 있다.\n",
        "# 평가를 한 네티즌 인원 수를 받아들여 공백을 지운다.\n",
        "# 총 리뷰 개수의 양쪽 괄호를 지우고 1000명이 넘으면 \n",
        "# 콤마를 제거해야 integer 전환시 에러가 없다.\n",
        "\n",
        "total = __TODO__\n",
        "\n",
        "print(f'총 리뷰 개수: {total:,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2vwe7TLtXml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 총 페이지 수를 계산한다.\n",
        "# 총 평점 개수를 10으로 나누고 정수로 변환해 총 페이지 개수를 계산한다.\n",
        "# 10으로 나누는 이유는 한 페이지당 평점이 10개씩 있어서 그렇다.\n",
        "# 총 페이지 수를 계산한 결과를 변수 page_no에 할당한다.\n",
        "\n",
        "page_no = __TODO__\n",
        "\n",
        "print(f'총 리뷰 페이지 수...: {page_no:,}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD6wSJSZtqix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grades = []                   # 태그를 제거한 리뷰 평점 전체를 담을 리스트를 초기화한다.\n",
        "reviews = []                  # 태그를 제거한 리뷰 전체를 담을 리스트를 초기화한다.\n",
        "\n",
        "try:\n",
        "    for i in range(__TODO__):  # 총 리뷰 페이지만큼 순환문을 사용해서 전체 리뷰를 가져온다.   \n",
        "        url = __TODO__\n",
        "        bs = BeautifulSoup(urlopen(url), 'html.parser')  # 'html5lib'\n",
        "\n",
        "        # --- find_all(name, attrs, recursive, string, limit, **kwargs)메소드를 통해 모든 리뷰와 평점을 추출한다.\n",
        "        # 태그를 제거한 해당 페이지의 네티즌 평점을 리스트에 추가한다.\n",
        "        grades += [grade.get_text() for grade in bs.find_all(__TODO__)]\n",
        "        # 태그를 제거한 해당 페이지의 네티즌 리뷰를 리스트에 추가한다.\n",
        "        reviews += [review.get_text(strip=True) for review in bs.find_all(__TODO__)]\n",
        "\n",
        "        # 진행 사항을 표시한다.\n",
        "        print('.', end='')\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f' 현재 가져온 리뷰 개수: {len(reviews):,}')\n",
        "    else:\n",
        "        print('\\n=======', 'Job completed!', '=' * 25)\n",
        "except urllib.error.HTTPError as err:\n",
        "    print('The server returned an HTTP error:', err)\n",
        "except urllib.error.URLError as err:\n",
        "    print('The server could not be found!', err)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUhLWU1wtyJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 수집한 리뷰의 수를 확인한다.\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5QqS6-UtzYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 수집한 리뷰 중 마지막 10개만 출력해본다.\n",
        "__TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIzkaG6Gt5lj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- 가져온 내용을 파일로 저장한다.\n",
        "path = 'movie-reviews-A.txt'\n",
        "\n",
        "# 텍스트 파일을 쓰기 모드로 연다.\n",
        "# 이 때 빈 리뷰와 평점은 파일로 저장하지 않는다.\n",
        "with open(path, mode='w', encoding='utf-8') as file:\n",
        "  __TODO__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Xg2kKb8O0IT",
        "colab": {}
      },
      "source": [
        "# ======= For Google Colaboratory ===============================\n",
        "# --- 수집한 데이터와 전처리한 데이터를 로컬 파일로 내려받기를 한다.\n",
        "from google.colab import files\n",
        "files.download(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZbP1gVfO0IV",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# <font color='red'>THE END</font>"
      ]
    }
  ]
}